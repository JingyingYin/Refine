{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "import os\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "sc = SparkContext(appName='entity_resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries for the asciifying task.\n",
    "group_a = {'a':[u'\\u00C0', u'\\u00C1', u'\\u00C2', u'\\u00C3', u'\\u00C4', u'\\u00C5', u'\\u00E0'\n",
    "                 , u'\\u00E1', u'\\u00E2', u'\\u00E3', u'\\u00E4', u'\\u00E5', u'\\u0100', u'\\u0101'\n",
    "                 , u'\\u0102', u'\\u0103', u'\\u0104', u'\\u0105']}\n",
    "group_c = {'c':[u'\\u00C7', u'\\u00E7', u'\\u0106', u'\\u0107', u'\\u0108', u'\\u0109', u'\\u010A'\n",
    "                 , u'\\u010B', u'\\u010C', u'\\u010D']}\n",
    "group_d = {'d':[u'\\u00D0', u'\\u00F0', u'\\u010E', u'\\u010F', u'\\u0110', u'\\u0111']}\n",
    "group_e = {'e':[u'\\u00C8', u'\\u00C9', u'\\u00CA', u'\\u00CB', u'\\u00E8', u'\\u00E9', u'\\u00EA'\n",
    "                 , u'\\u00EB', u'\\u0112', u'\\u0113', u'\\u0114', u'\\u0115', u'\\u0116', u'\\u0117'\n",
    "                 , u'\\u0118', u'\\u0119', u'\\u011A', u'\\u011B']}\n",
    "group_g = {'g':[u'\\u011C', u'\\u011D', u'\\u011E', u'\\u011F', u'\\u0120', u'\\u0121', u'\\u0122'\n",
    "                 , u'\\u0123']}\n",
    "group_h = {'h':[u'\\u0124', u'\\u0125', u'\\u0126', u'\\u0127']}\n",
    "group_i = {'i':[u'\\u00CC', u'\\u00CD', u'\\u00CE', u'\\u00CF', u'\\u00EC', u'\\u00ED', u'\\u00EE'\n",
    "                 , u'\\u00EF', u'\\u0128', u'\\u0129', u'\\u012A', u'\\u012B', u'\\u012C', u'\\u012D'\n",
    "                 , u'\\u012E', u'\\u012F', u'\\u0130', u'\\u0131']}\n",
    "group_j = {'j':[u'\\u0134', u'\\u0135']}\n",
    "group_k = {'k':[u'\\u0136', u'\\u0137', u'\\u0138']}\n",
    "group_l = {'l':[u'\\u0139', u'\\u013A', u'\\u013B', u'\\u013C', u'\\u013D', u'\\u013E', u'\\u013F'\n",
    "                 , u'\\u0140', u'\\u0141', u'\\u0142']}\n",
    "group_n = {'n':[u'\\u00D1', u'\\u00F1', u'\\u0143', u'\\u0144', u'\\u0145', u'\\u0146', u'\\u0147'\n",
    "                 , '\\u0148', '\\u0149', '\\u014A', '\\u014B']}\n",
    "group_o = {'o':[u'\\u00D2', u'\\u00D3', u'\\u00D4', u'\\u00D5', u'\\u00D6', u'\\u00D8', u'\\u00F2'\n",
    "                 , u'\\u00F3', u'\\u00F4', u'\\u00F5', u'\\u00F6', u'\\u00F8', u'\\u014C', u'\\u014D'\n",
    "                 , u'\\u014E', u'\\u014F', u'\\u0150', u'\\u0151']}\n",
    "group_r = {'r':[u'\\u0154', u'\\u0155', u'\\u0156', u'\\u0157', u'\\u0158', u'\\u0159']}\n",
    "group_s = {'s':[u'\\u015A', u'\\u015B', u'\\u015C', u'\\u015D', u'\\u015E', u'\\u015F', u'\\u0160'\n",
    "                 , u'\\u0161', u'\\u017F']}\n",
    "group_t = {'t':[u'\\u0162', u'\\u0163', u'\\u0164', u'\\u0165', u'\\u0166', u'\\u0167']}\n",
    "group_u = {'u':[u'\\u00D9', u'\\u00DA', u'\\u00DB', u'\\u00DC', u'\\u00F9', u'\\u00FA', u'\\u00FB'\n",
    "                 , u'\\u00FC', u'\\u0168', u'\\u0169', u'\\u016A', u'\\u016B', u'\\u016C', u'\\u016D'\n",
    "                 , u'\\u016E', u'\\u016F', u'\\u0170', u'\\u0171', u'\\u0172', u'\\u0173']}\n",
    "group_w = {'w':[u'\\u0174', u'\\u0175']}\n",
    "group_y = {'y':[u'\\u00DD', u'\\u00FD', u'\\u00FF', u'\\u0176', u'\\u0177', u'\\u0178']}\n",
    "group_z = {'z':[u'\\u0179', u'\\u017A', u'\\u017B', u'\\u017C', u'\\u017D', u'\\u017E']}\n",
    "\n",
    "group_list = [group_a, group_c, group_d, group_e, group_g, group_h, group_i, group_j,\n",
    "             group_k, group_l, group_n, group_o, group_r, group_s, group_t, group_u,\n",
    "             group_w, group_y, group_z]\n",
    "translate_dict = {}\n",
    "for group in group_list:\n",
    "    return_value = group.keys()[0]\n",
    "    for char in group[return_value]:\n",
    "        translate_dict[char] = return_value\n",
    "sorted(translate_dict.items(), key=lambda kv: kv[1])\n",
    "#pickle_out('translate_dict', translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityResolution:\n",
    "    \"\"\"\n",
    "    Authors: Myoungsu Choi (myoungsu@usc.edu), \n",
    "             Jingying Yin (yinjingy@usc.edu),\n",
    "             Yining Zhang (yzhang27@usc.edu)\n",
    "    \"\"\"\n",
    "    \n",
    "    default_parameters = {\n",
    "        'method': 'fingerprint',\n",
    "        'partition_num': 8,\n",
    "        'default_file_path': './sub_result/', \n",
    "        'signature_len': 15,\n",
    "        'band_size': 5,\n",
    "        'annotate': False\n",
    "    }\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_file_path=None,\n",
    "                 partition_num=default_parameters['partition_num'],\n",
    "                 method=default_parameters['method'],\n",
    "                 signature_len=default_parameters['signature_len'],\n",
    "                 band_size=default_parameters['band_size'],\n",
    "                 default_file_path=default_parameters['default_file_path'],\n",
    "                 annotate=default_parameters['annotate'],\n",
    "                 n=2):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_file_path: String, a path of the input file. \n",
    "        partition_num: Integer, a number of the partition for spark.\n",
    "        method: String, a method for the entity resolution.\n",
    "        signature_len: Integer, the row length of the signature matix.\n",
    "        band_size: Integer, a band size of the LSH.\n",
    "        default_file_path: String, a default file path for the pickling in and out.\n",
    "        annotate: Boolean, whether to print the tree or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        method_available = ['fingerprint', 'ngram_fingerprint',\n",
    "                            'fingerprint_lhs', 'ngram_fingerprint_lhs']\n",
    "        method_mapper = {\n",
    "            'fingerprint':self.extract_fingerprint,\n",
    "            'ngram_fingerprint':self.extract_ngram_fingerprint,\n",
    "        }\n",
    "        \n",
    "        if not input_file_path:\n",
    "            raise ValueError('Input_file_path should be given.')\n",
    "        if method not in method_available:\n",
    "            raise ValueError('Available methods are ' + str(method_available))\n",
    "            \n",
    "        self.method = method\n",
    "        self.partition_num = partition_num\n",
    "        self.input_file_path = input_file_path\n",
    "        self.default_file_path = default_file_path\n",
    "        self.signature_len = signature_len\n",
    "        self.band_size = band_size\n",
    "        self.banded_output = False\n",
    "        translate_dict = self.pickle_in('translate_dict')\n",
    "        \n",
    "        self.func = method_mapper[self.method]\n",
    "        self.translate_dict = translate_dict\n",
    "        self.non_ascii_set = set(translate_dict.keys())\n",
    "        self.n = n\n",
    "        self.annotate = annotate\n",
    "        \n",
    "        if self.method == 'fingerprint':\n",
    "            self.preprocess_func = self.make_tokens\n",
    "        elif self.method == 'ngram_fingerprint':\n",
    "            self.preprocess_func = self.make_ngrams\n",
    "        \n",
    "    def pickle_out(self, name, obj):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: String, a file name for the saving in pickle object.\n",
    "        obj: Object, an object for the saving\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.default_file_path):\n",
    "            os.makedirs(self.default_file_path)\n",
    "        pickle_out = open(self.default_file_path + name + '.pickle','wb')\n",
    "        pickle.dump(obj, pickle_out)\n",
    "        pickle_out.close()\n",
    "        return\n",
    "\n",
    "    def pickle_in(self, name):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: String, a file name for the saving in pickle object.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A loaded object.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.default_file_path):\n",
    "            raise ValueError(self.default_file_path+' is not existed.')\n",
    "        pickle_in = open(self.default_file_path + name + '.pickle','rb')\n",
    "        obj = pickle.load(pickle_in)\n",
    "        pickle_in.close()\n",
    "        return obj\n",
    "    \n",
    "    # Abstract funtion\n",
    "    # Implemented in EntityResolutionLsh\n",
    "    def make_tokens(self, string):\n",
    "        return\n",
    "    \n",
    "    # Abstract funtion\n",
    "    # Implemented in EntityResolutionLsh\n",
    "    def make_ngrams(self, string):\n",
    "        return\n",
    "    \n",
    "    def strip(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the strip task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A string after removing leading and trailing whitespace.\n",
    "        \"\"\"\n",
    "        return string.strip()\n",
    "\n",
    "    def lower(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the lower task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A string in a lowercase representation form\n",
    "        \"\"\"\n",
    "        return string.lower()\n",
    "\n",
    "    def remove_punctuation(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the removing punctuation task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A string after removing punctuation.\n",
    "        \"\"\"\n",
    "        pattern = r'[^\\w\\s]'\n",
    "        return re.sub(pattern, '', string)\n",
    "    \n",
    "    def remove_whitespace(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the removing whitespace task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A string after removing whitespace in any position of the string.\n",
    "        \"\"\"\n",
    "        pattern = r'[\\s]'\n",
    "        return re.sub(pattern, '', string)\n",
    "    \n",
    "    def asciify(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the asciifying task.\n",
    "        [Asciifying task: normalizing extended western characters to their ASCII representation]\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A string after asciifying task.\n",
    "        \"\"\"\n",
    "        if not isinstance(string, unicode):\n",
    "            string = unicode(string, 'utf-8')\n",
    "        return_str = ''\n",
    "        for char in string:\n",
    "            if char in self.non_ascii_set:\n",
    "                return_str += self.translate_dict[char]\n",
    "            else:\n",
    "                return_str += char\n",
    "        return str(return_str)\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the tokenizing task based on the whitespace.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A list of the tokens after tokenizing.\n",
    "        \"\"\"\n",
    "        return string.split()\n",
    "\n",
    "    def remove_duplicates(self, token_list):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        token_list: List, a list for the removing the duplicates in the list.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A list of the tokens after the removing dupliacates.\n",
    "        \"\"\"\n",
    "        token_series = pd.Series(token_list)\n",
    "        return list(token_series.unique())\n",
    "\n",
    "    def sort_list(self, token_list):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        token_list: List, a list for the sorting task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A list of the tokens after the sorting task.\n",
    "        \"\"\"\n",
    "        return sorted(token_list, reverse=False)\n",
    "\n",
    "    def join_string(self, token_list, ngram=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        token_list: List, a list for the joining all the tokens based on the whitespace.\n",
    "        ngram: Boolean, an indicator whether method is ngram based or not. \n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A string after the joining all the tokens based on whitespace.\n",
    "        \"\"\"\n",
    "        if not ngram:\n",
    "            return ' '.join(token_list)\n",
    "        else:\n",
    "            temp_list = []\n",
    "            for tuple_item in token_list:\n",
    "                for i in range(len(tuple_item)):\n",
    "                    temp_list.append(tuple_item[i])\n",
    "            return ''.join(temp_list)\n",
    "\n",
    "    def get_ngram(self, token_list, n=2):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        token_list: List, a list for getting a ngram list.\n",
    "        n: Integer, an number of near items gathered during the ngram.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A list of the ngram items.\n",
    "        \"\"\"\n",
    "        return list(ngrams(token_list, n))\n",
    "\n",
    "    def extract_fingerprint(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the extracting a fingerprint.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A fingerprint based on the fingerprinting method.\n",
    "        \"\"\"\n",
    "        string = self.strip(string)\n",
    "        string = self.lower(string)\n",
    "        string = self.remove_punctuation(string)\n",
    "        string = self.asciify(string)\n",
    "        token_list = self.tokenize(string)\n",
    "        token_list = self.sort_list(token_list)\n",
    "        token_list = self.remove_duplicates(token_list)\n",
    "        fingerprint = self.join_string(token_list)\n",
    "        fingerprint = self.strip(fingerprint)\n",
    "        return fingerprint\n",
    "\n",
    "    def extract_ngram_fingerprint(self, string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        string: String, a string for the extracting a fingerprint based on ngram.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A fingerprint based on the ngram-fingerprinting method.\n",
    "        \"\"\"\n",
    "        string = self.lower(string)\n",
    "        string = self.remove_punctuation(string)\n",
    "        string = self.remove_whitespace(string)\n",
    "        ngram_list = self.get_ngram(list(string), n=self.n)\n",
    "        ngram_list = self.sort_list(ngram_list)\n",
    "        ngram_list = self.remove_duplicates(ngram_list)\n",
    "        fingerprint = self.join_string(ngram_list, ngram=True)\n",
    "        fingerprint = self.asciify(fingerprint)\n",
    "        return fingerprint\n",
    "    \n",
    "    def get_normalized_entity(self, entity_list):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entity_list: List, a list of the entities for the extracting fingerprint task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A list of the tuples of fingerprint and original entity string.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for entity in entity_list:\n",
    "            normalized_entity = self.func(entity)\n",
    "            result.append((normalized_entity, entity))\n",
    "        return result\n",
    "    \n",
    "    def clustering_entity(self, column=None, sc=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        column: String, a name of the column of the DataFrame, which is supposed to be worked on.\n",
    "        sc: SparkContext instance.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A DataFrame that includes 3 columns(fingerprint, original column, newly assigned column).\n",
    "        \"\"\"\n",
    "        time1 = time.time()\n",
    "        if not isinstance(column, str):\n",
    "            raise ValueError('column should be string.')\n",
    "        if not sc:\n",
    "            raise ValueError('Spark Context should be given.')\n",
    "        df_raw = pd.read_csv(self.input_file_path) \n",
    "        \n",
    "        if column not in df_raw.columns:\n",
    "            raise ValueError(column +' is not in the column list.')\n",
    "        series_raw = df_raw[column]\n",
    "        lines = sc.parallelize(series_raw, self.partition_num)\n",
    "        \n",
    "        entity_fingerprint_list = lines.mapPartitions(self.get_normalized_entity).collect()\n",
    "                 \n",
    "        df_result = pd.DataFrame(data=entity_fingerprint_list, \n",
    "                                 columns=['fingerprint', 'old_entity'])\n",
    "        \n",
    "        unique_list = df_result.groupby('fingerprint')['old_entity'].nunique()\n",
    "        cluster_set = set(unique_list[unique_list>1].index)\n",
    "        df_clustered = df_result[df_result['fingerprint'].apply(lambda x: x in cluster_set)]\n",
    "        series_new_entity = df_clustered.groupby('fingerprint')['old_entity']\\\n",
    "                            .agg(lambda x: x.value_counts().index[0])\n",
    "        series_new_entity.name = 'new_entity'\n",
    "        \n",
    "        df_output = \\\n",
    "            pd.merge(df_result, series_new_entity, \n",
    "                     how='left', left_on='fingerprint', right_index=True)\n",
    "        df_output['new_entity'].fillna(df_output['old_entity'], inplace=True)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        if self.annotate:\n",
    "            print '• Function took %0.1f sec' % ((time2-time1))\n",
    "            print '• '+ str(len(df_output.query('old_entity!=new_entity')['fingerprint'].unique())) \\\n",
    "                  + ' clusters founded.'\n",
    "        return df_output\n",
    "    \n",
    "class EntityResolutionLsh(EntityResolution):\n",
    "            \n",
    "    def make_tokens(self, init_string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        init_string: String, an original entity.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A tuple of original string and a list of tokens.\n",
    "        \"\"\"\n",
    "        string = self.strip(init_string)\n",
    "        string = self.lower(string)\n",
    "        string = self.remove_punctuation(string)\n",
    "        string = self.asciify(string)\n",
    "        token_list = self.tokenize(string)\n",
    "        token_list = self.sort_list(token_list)\n",
    "        token_list = self.remove_duplicates(token_list)\n",
    "        return (init_string, token_list) \n",
    "    \n",
    "    def make_ngrams(self, init_string):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        init_string: String, an original entity.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A tuple of original string and a list of ngrams.\n",
    "        \"\"\"\n",
    "        string = self.lower(init_string)\n",
    "        string = self.remove_punctuation(string)\n",
    "        string = self.remove_whitespace(string)\n",
    "        ngram_list = self.get_ngram(list(string), n=self.n)\n",
    "        ngram_list = self.sort_list(ngram_list)\n",
    "        ngram_list = self.remove_duplicates(ngram_list)\n",
    "        return (init_string, ngram_list)\n",
    "    \n",
    "    def get_preprocessed_entity(self, entity_list):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entity_list: List, a list of the entities for the preprocessing task for the LSH task.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A list of the tuples from the make_ngrams or the make_tokens function.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for entity in entity_list:\n",
    "            preprocessed_entity = self.preprocess_func(entity)\n",
    "            result.append(preprocessed_entity)\n",
    "        return result\n",
    "    \n",
    "    def hash_func(self, mid, i):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        mid: Integer, token or ngram id from the token2id.\n",
    "        i: Integer, indices for the hashing [Scope: (1, signature_len)].\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        An hashed integer. \n",
    "        \"\"\"\n",
    "        return (3*mid + 11*i) % 100 + 1\n",
    "\n",
    "    def calc_jaccard(self, list1, list2):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        list1, list2: List, lists for the calculating jaccard score.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A jaccard score.\n",
    "        \"\"\"\n",
    "        set1 = set(list1)\n",
    "        set2 = set(list2)\n",
    "        union = set1 | set2\n",
    "        intersect = set1 & set2\n",
    "        return len(intersect)/len(union)\n",
    "    \n",
    "    def band2idx(self, band_num, band_size, signature_len):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        band_num: Integer, an index of the band.\n",
    "        band_size: Integer, the size of the band for the LSH task.\n",
    "        signature_len: Integer, the row length of the signature matix.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        A tuple of start index and end index for the making banded minhash matrix.\n",
    "        \"\"\"\n",
    "        if band_num > band_size:\n",
    "            raise ValueError('band_num cannot be larger than band_size')\n",
    "        unit = signature_len/band_size\n",
    "        start_idx = band_num*unit\n",
    "        end_idx = start_idx + unit\n",
    "        return (start_idx, end_idx)\n",
    "    \n",
    "    def minhash(self, iterator):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        iterator: List, \n",
    "                  a list of (entity, [token1, token2, ,,,]) or (entity, [ngram1, ngram2, ,,,])\n",
    "                  characteristic matrix\n",
    "        Return\n",
    "        ======\n",
    "        A list of the tuples. (original entity, a signature column[list type])\n",
    "        • banded_output: (band_index, (original entity, a signature column[list type]))\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for entity in iterator:\n",
    "            signature_col = np.full(self.signature_len, np.inf)\n",
    "            for token in entity[1]:\n",
    "                for i in range(self.signature_len):\n",
    "                    hashed_idx = self.hash_func(self.token2id[token], i+1)\n",
    "                    if hashed_idx < signature_col[i]:\n",
    "                        signature_col[i] = hashed_idx\n",
    "            if self.banded_output:\n",
    "                for band in range(self.band_size):\n",
    "                    start_idx, end_idx = self.band2idx(band, \n",
    "                                                       self.band_size, \n",
    "                                                       self.signature_len)\n",
    "                    result.append((band, (entity[0], list(signature_col)[start_idx:end_idx])))\n",
    "            else:\n",
    "                result.append((entity[0], list(signature_col)))\n",
    "        return result\n",
    "\n",
    "    def make_bucket(self, iterator):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        iterator: List, a banded minhash matrix.\n",
    "        \n",
    "        Return\n",
    "        ======\n",
    "        A list of the tuples of band_num and \n",
    "        a dictionary of (key:hashed_value, value: a list of entities that shares the same hash.)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for band in iterator:\n",
    "            itermediate_dict = {}\n",
    "            for item in band[1]:\n",
    "                hashed_value = hash(str(item[1]))\n",
    "                if not hashed_value in itermediate_dict.keys():\n",
    "                    itermediate_dict[hashed_value] = set([item[0]])\n",
    "                else:\n",
    "                    itermediate_dict[hashed_value] = itermediate_dict[hashed_value] | set([item[0]])\n",
    "            result.append((band[0], itermediate_dict))\n",
    "        return result\n",
    "\n",
    "    def find_candidates(self, band, entity):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        band: Integer, an index of the band\n",
    "        entity: String, the original entity.\n",
    "        \n",
    "        Return\n",
    "        ======\n",
    "        A list of candidates for the similar entities. \n",
    "        \"\"\"\n",
    "        candidates_list = set()\n",
    "        for similar_entity_set in band[1].values():\n",
    "            if entity in similar_entity_set:\n",
    "                similar_entity_set = similar_entity_set - set([entity])\n",
    "                candidates_list = candidates_list | similar_entity_set\n",
    "        return list(candidates_list)\n",
    "\n",
    "    def find_similar_entity(self, \n",
    "                            entity,\n",
    "                            candidates_list, \n",
    "                            minhash_matrix, \n",
    "                            top=1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entity: String, the original entity.\n",
    "        candidates_list: List, a list of candidates for the similar entities. \n",
    "        minhash_matrix: List, a list of the tuples. \n",
    "                        (original entity, a signature column[list type])\n",
    "        top: Integer, a number of how many similar entities will be returned.\n",
    "        \n",
    "        Return\n",
    "        ======\n",
    "        A list of similar entities.\n",
    "        \"\"\"\n",
    "        if candidates_list <= top:\n",
    "            return candidates_list\n",
    "        else:\n",
    "            entity_col = minhash_matrix[entity]\n",
    "            similar_entity_list = []\n",
    "            for cand in candidates_list:\n",
    "                candidate_col = minhash_matrix[cand]\n",
    "                jaccard_score = self.calc_jaccard(entity_col, candidate_col)\n",
    "                similar_entity_list.append((jaccard_score, cand))\n",
    "            similar_entity_list = sorted(similar_entity_list, reverse=True)\n",
    "            similar_entity_list = [entity for score, entity in similar_entity_list]\n",
    "        return similar_entity_list[:top]\n",
    "    \n",
    "    def make_token2id_id2token(self, preprocessed_lines):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        preprocessed_lines: List, a list of the tuples (init_string, token_list or ngram_list)\n",
    "                            This is from self.preprocess_func \n",
    "                            [self.make_tokens or self.make_ngrams]\n",
    "        \n",
    "        \"\"\"\n",
    "        all_tokens = []\n",
    "        for item in preprocessed_lines:\n",
    "            all_tokens.extend(item[1])\n",
    "        all_tokens = pd.Series(all_tokens).unique()\n",
    "        self.token2id = {item:i for i, item in enumerate(all_tokens)}\n",
    "        self.id2token = {i:item for i, item in enumerate(all_tokens)}\n",
    "        return \n",
    "    \n",
    "    def predict(self, entity_list):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entity_list: List, a list of the original entities that are supposed to be worked on.\n",
    "        \n",
    "        Return\n",
    "        ======\n",
    "        A list of the tuples of original entity and newly assigned entity.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for entity in entity_list:\n",
    "            candidates_list = []\n",
    "            for band in self.buckets:\n",
    "                candidates_list.extend(self.find_candidates(band, entity))\n",
    "            candidates_list = list(set(candidates_list))\n",
    "            similar_entity = self.find_similar_entity(entity, \n",
    "                                                      candidates_list, \n",
    "                                                      self.minhash_matrix)\n",
    "            result.append((entity, similar_entity))\n",
    "        return result\n",
    "    \n",
    "    def clustering_entity(self, column=None, sc=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        column: String, a name of the column of the DataFrame, which is supposed to be worked on.\n",
    "        sc: SparkContext.\n",
    "        \n",
    "        Return\n",
    "        ======\n",
    "        A list of the tuples of original entity and newly assigned entity.\n",
    "        \"\"\"\n",
    "        time1 = time.time()\n",
    "        if not isinstance(column, str):\n",
    "            raise ValueError('column should be string.')\n",
    "        if not sc:\n",
    "            raise ValueError('Spark Context should be given.')\n",
    "            \n",
    "        df_raw = pd.read_csv(self.input_file_path) \n",
    "        if column not in df_raw.columns:\n",
    "            raise ValueError(column +' is not in the column list.')\n",
    "        series_raw = df_raw[column]\n",
    "        lines = sc.parallelize(series_raw, self.partition_num)\n",
    "        \n",
    "        preprocessed_lines = lines.mapPartitions(self.get_preprocessed_entity).collect()\n",
    "        self.make_token2id_id2token(preprocessed_lines)\n",
    "        \n",
    "        prep_lines_serial = sc.parallelize(preprocessed_lines, self.partition_num)\n",
    "        \n",
    "        self.banded_output = True\n",
    "        minhash_banded_matrix = prep_lines_serial\\\n",
    "                                .mapPartitions(self.minhash)\\\n",
    "                                .collect()\n",
    "        self.banded_output = False\n",
    "        minhash_matrix = prep_lines_serial\\\n",
    "                        .mapPartitions(self.minhash).collect()\n",
    "        minhash_matrix = {entity:column for entity, column in minhash_matrix}\n",
    "        self.minhash_matrix = minhash_matrix\n",
    "        \n",
    "        buckets = sc.parallelize(minhash_banded_matrix, self.partition_num)\n",
    "        buckets = buckets.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "        buckets = buckets.mapPartitions(self.make_bucket).collect()\n",
    "        self.buckets = buckets\n",
    "        \n",
    "        pred_result = sc.parallelize(series_raw, self.partition_num)\n",
    "        pred_result = pred_result.mapPartitions(self.predict)\\\n",
    "                      .collect()\n",
    "        \n",
    "        time2 = time.time()\n",
    "        if self.annotate:\n",
    "            print '• Function took %0.1f sec' % ((time2-time1))\n",
    "        return pred_result\n",
    "\n",
    "\n",
    "# df_ratings = pd.read_csv(input_file_path)\n",
    "# df_ratings_sample = df_ratings.sample(10000)\n",
    "# df_ratings_sample.to_csv(sample_file_path, index=False)\n",
    "# df_sample = pd.read_csv(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = './data/restaurant-and-market-health-inspections.csv'\n",
    "sample_file_path = './data/sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = EntityResolution(input_file_path=input_file_path, \n",
    "                      method='fingerprint',\n",
    "                      partition_num=16,\n",
    "                      annotate=True,\n",
    "                      n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Function took 10.7 sec\n",
      "• 107 clusters founded.\n"
     ]
    }
   ],
   "source": [
    "result = er.clustering_entity(column='facility_name', sc=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference\n",
    "    - [1 item]: [MCDONALD'S  #1126]<->[MCDONALD’S #1126] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44327    MCDONALD'S  #1126\n",
       "51836     MCDONALD’S #1126\n",
       "53499     MCDONALD’S #1126\n",
       "55752     MCDONALD’S #1126\n",
       "65706    MCDONALD'S  #1126\n",
       "Name: old_entity, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['fingerprint']=='1126 mcdonalds']['old_entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_lsh = EntityResolutionLsh(input_file_path=sample_file_path, \n",
    "                             method='fingerprint',\n",
    "                             partition_num=16,\n",
    "                             signature_len=20,\n",
    "                             band_size=10,\n",
    "                             annotate=True,\n",
    "                             n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_lsh.clustering_entity(column='facility_name', sc=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
